# Grading-Operations-Performance-Dashboard

## Project Background 

###  Project Request: Grading Operations Performance Dashboard
### Requesting Team: Operations & Quality Improvement Team
### Point of Contact: Operations Analyst (Prospective Candidate Project)

**Project Scenario:**

Grading Company’s leadership team needs better visibility into key grading operational metrics. Delays in turnaround times, inconsistent error rates, and varying customer feedback scores are impacting customer satisfaction and operational efficiency. Additionally, leadership wants deeper insights into how service level, turnaround time, and operational volume impact quality, revenue, and customer experience.

The operations team is requesting a dashboard that not only tracks core KPIs but also highlights trends and relationships between key metrics to uncover actionable insights for process improvements and resource planning.


**Project Objectives:**

* Operational Efficiency: Track average turnaround times, trends by service level, and identify whether faster services deliver consistent results.
* Quality Assurance: Measure overall error rates and compare them across service levels and over time.
* Customer Experience: Monitor feedback scores and analyze how satisfaction changes based on turnaround time.
* Revenue Insights: Understand revenue contributions by service level and extras to identify upsell opportunities.


**Expected Impact:**

* Provide leadership with data-driven insights that go beyond static KPIs, showing relationships between speed, quality, revenue, and customer satisfaction.
* Reveal which service levels generate the most revenue and maintain high quality, guiding decisions on pricing and promotions.
* Identify when operational strain affects error rates or customer satisfaction, supporting staffing and SOP adjustments during peak times.
* Deliver a stakeholder-ready dashboard that combines trends, breakdowns, and insights to inform operational improvements and strategic planning directly.


---

## Key Performance Indicators (KPIs)

### **Turnaround Times / Service Level**
- **Average Turnaround Time (Days)** – Tracks the average number of days taken to complete grading.
- **Submissions by Month and Service Level** – Shows monthly submission volume broken down by service level.
- **Turnaround by Service Level (Days)** – Compares average turnaround times across different service levels.

---

### **Error Rate**
- **Error Rate (%)** – Percentage of submissions with errors.
- **Total Submissions vs Total Errors Per Month** – Tracks errors relative to total submissions over time.
- **Error Rate by Service Level** – Compares error rates across service tiers.

---

### **Customer Feedback**
- **Average Feedback Score (Out of 5)** – Average customer satisfaction rating.
- **Feedback by Turnaround Time** – Correlates feedback scores with turnaround time buckets.
- **Feedback by Turnaround Time and Service Level** – Shows how turnaround and service tier affect customer feedback.

---

### **Revenue Opportunities**
- **Total Revenue (USD)** – Total revenue generated.
- **Revenue (%) by Service Level** – Service-level contribution to total revenue.
- **Revenue (%) by Total Submissions and Service Level** – Revenue share adjusted for submission volume.

## Business Insights

### **Turnaround Time (TAT) + Service Level**
- TATs by service level are generally aligned with estimated completion dates.
- In this example, submissions are evenly balanced across all service levels.
- If **Base Tier** services experience higher volumes, bottlenecks could occur in overall operations.
- Consider introducing **incentives** for customers to upgrade to higher tiers when Base Tier volume increases.
- With similar volumes across service levels, **adapting SOPs** from Express into Base Tier could reduce TAT without significantly increasing costs.
- Note: This “perfect” distribution is likely a result of the AI-generated dataset lacking real-world variability.

---

### **Error Rate**
- **Priority** and **Base (Without Subgrades)** are the worst performers in error rate.
- Base (Without Subgrades) may suffer from lower complexity, leading to QA oversight or being assigned to less experienced graders.
- Ensure **Priority SOPs** maintain quality without sacrificing accuracy for speed.
- **Express** has the lowest error rate, a best-practice benchmark for other service levels, especially Priority.
- Errors occurred most frequently **early in the year**, possibly due to:
  - SOP updates after New Year
  - Post-holiday submission spikes
  - Staff fatigue following holidays

---

### **Customer Feedback**
- Average feedback is **consistent** across turnaround times.
- This suggests either:
  - All service levels are delivering within expected timeframes, or
  - Turnaround time is **not** the main driver of customer satisfaction.
- Higher feedback appears in the **15+ day TAT** for certain tiers (e.g., Priority), suggesting customers may value **quality and accuracy over speed**.
- Feedback in the **8–14 day range** is the lowest, possibly a “gray zone” perceived as **too quick to be thorough** but **too slow to be fast service**.

---

### **Revenue Opportunities**
- Customers value **subgrades** and are willing to pay more for detailed grading.
- Revenue is directly correlated with submission volume (assuming uniform pricing across services).
- Promote **efficient service lines** and consider **cross-selling** during peak times for each service level.
- **Base Service (No Subgrades)** has low volume and low revenue, possible strategies:
  - Price adjustments
  - Bundling or bulk service offerings
  - Removing this tier altogether to maintain **BGS prestige**, given BGS and BCCG already exist.
